{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f888769",
   "metadata": {},
   "source": [
    "# üîç QR Code Model Evaluation - Simple & Fast\n",
    "\n",
    "**Quick evaluation of your trained model**\n",
    "\n",
    "## üìã Prerequisites on Kaggle:\n",
    "1. Add your model dataset: `/kaggle/input/qr-fishing/pytorch/default/1/best_model.pth`\n",
    "2. Add QR codes dataset: `benign-and-malicious-qr-codes`\n",
    "3. Enable GPU (optional, but faster)\n",
    "4. **Run all cells** (Ctrl+A, then Shift+Enter)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: Install & Import (Run this first!)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image, ImageFile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, \n",
    "    precision_recall_fscore_support, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print('‚úÖ Imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: Configuration - UPDATE THESE PATHS IF NEEDED!\n",
    "# ============================================================================\n",
    "\n",
    "# üîß CHANGE THESE IF YOUR PATHS ARE DIFFERENT\n",
    "MODEL_PATH = '/kaggle/input/qr-fishing/pytorch/default/1/best_model.pth'\n",
    "DATA_DIR = '/kaggle/input/benign-and-malicious-qr-codes/QR codes'\n",
    "OUTPUT_DIR = '/kaggle/working'\n",
    "\n",
    "# Model config (must match training)\n",
    "IMG_SIZE = 256\n",
    "MODEL_NAME = 'efficientnet_b3'\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'üî• Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('   ‚ö†Ô∏è No GPU - evaluation will be slower')\n",
    "\n",
    "# Verify paths\n",
    "print(f'\\nüìÇ Checking paths...')\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(f'   ‚úÖ Model found: {MODEL_PATH}')\n",
    "else:\n",
    "    print(f'   ‚ùå Model NOT found: {MODEL_PATH}')\n",
    "    print(f'   üí° Add your model dataset in Kaggle!')\n",
    "\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f'   ‚úÖ Data found: {DATA_DIR}')\n",
    "else:\n",
    "    print(f'   ‚ùå Data NOT found: {DATA_DIR}')\n",
    "    print(f'   üí° Add the QR codes dataset in Kaggle!')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f'\\n‚úÖ Configuration complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: Define Model Architecture (Must match training!)\n",
    "# ============================================================================\n",
    "\n",
    "class QRClassifier(nn.Module):\n",
    "    def __init__(self, model_name='efficientnet_b3', dropout_rate=0.3, hidden_units=256):\n",
    "        super(QRClassifier, self).__init__()\n",
    "        \n",
    "        if model_name == 'efficientnet_b3':\n",
    "            self.backbone = models.efficientnet_b3(pretrained=False)\n",
    "        elif model_name == 'efficientnet_b2':\n",
    "            self.backbone = models.efficientnet_b2(pretrained=False)\n",
    "        elif model_name == 'efficientnet_b0':\n",
    "            self.backbone = models.efficientnet_b0(pretrained=False)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported model: {model_name}')\n",
    "        \n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(in_features, hidden_units),\n",
    "            nn.BatchNorm1d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_rate/2),\n",
    "            nn.Linear(hidden_units, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "print('‚úÖ Model architecture defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: Load Pre-trained Model\n",
    "# ============================================================================\n",
    "\n",
    "print(f'üì¶ Loading model from: {MODEL_PATH}\\n')\n",
    "\n",
    "try:\n",
    "    # Initialize model\n",
    "    model = QRClassifier(model_name=MODEL_NAME, dropout_rate=0.3, hidden_units=256).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f'‚úÖ Model loaded successfully!')\n",
    "    print(f'   Trained epochs: {checkpoint.get(\"epoch\", \"Unknown\")}')\n",
    "    print(f'   Best val accuracy: {checkpoint.get(\"val_acc\", 0):.4f}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error loading model: {e}')\n",
    "    print(f'\\nüí° Troubleshooting:')\n",
    "    print(f'   1. Check if MODEL_PATH is correct in Step 2')\n",
    "    print(f'   2. Make sure you added the model dataset in Kaggle')\n",
    "    print(f'   3. Verify MODEL_NAME matches your trained model')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: Prepare Test Data\n",
    "# ============================================================================\n",
    "\n",
    "class QRDataset(Dataset):\n",
    "    def __init__(self, file_label_pairs, transform=None):\n",
    "        self.files = [p for p, _ in file_label_pairs]\n",
    "        self.labels = [lbl for _, lbl in file_label_pairs]\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = Image.open(self.files[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.labels[idx]\n",
    "        except Exception:\n",
    "            if self.transform:\n",
    "                black_img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "                return self.transform(black_img), self.labels[idx]\n",
    "            return torch.zeros(3, IMG_SIZE, IMG_SIZE), self.labels[idx]\n",
    "\n",
    "# Transform\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Collect images\n",
    "print('üìÇ Loading test images...')\n",
    "benign_dir = os.path.join(DATA_DIR, 'benign', 'benign')\n",
    "malicious_dir = os.path.join(DATA_DIR, 'malicious', 'malicious')\n",
    "\n",
    "image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".tiff\", \".webp\"}\n",
    "\n",
    "benign_files = [(str(f), 0) for ext in image_extensions \n",
    "                for f in Path(benign_dir).glob(f'**/*{ext}')]\n",
    "malicious_files = [(str(f), 1) for ext in image_extensions \n",
    "                   for f in Path(malicious_dir).glob(f'**/*{ext}')]\n",
    "\n",
    "all_files = benign_files + malicious_files\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# Use 10% as test set\n",
    "test_size = int(len(all_files) * 0.10)\n",
    "test_pairs = all_files[:test_size]\n",
    "\n",
    "print(f'‚úÖ Test set: {len(test_pairs):,} images')\n",
    "print(f'   Benign: {len(benign_files):,}')\n",
    "print(f'   Malicious: {len(malicious_files):,}')\n",
    "\n",
    "# Create dataloader\n",
    "test_dataset = QRDataset(test_pairs, transform=test_transform)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'‚úÖ DataLoader ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a8609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: Run Evaluation (This takes 2-5 minutes)\n",
    "# ============================================================================\n",
    "\n",
    "print('üîç Evaluating model on test set...\\n')\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc='Testing'):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        predicted = (probs >= 0.5).float()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Convert to numpy\n",
    "test_preds = np.array(all_preds).flatten()\n",
    "test_labels = np.array(all_labels).flatten()\n",
    "test_probs = np.array(all_probs).flatten()\n",
    "\n",
    "test_preds_binary = (test_probs >= 0.5).astype(int)\n",
    "test_labels_binary = test_labels.astype(int)\n",
    "\n",
    "print('\\n‚úÖ Evaluation complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: Calculate Metrics & Show Results\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate all metrics\n",
    "accuracy = (test_preds_binary == test_labels_binary).mean()\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    test_labels_binary, test_preds_binary, average='binary'\n",
    ")\n",
    "roc_auc = roc_auc_score(test_labels_binary, test_probs)\n",
    "avg_precision = average_precision_score(test_labels_binary, test_probs)\n",
    "\n",
    "# Class-wise metrics\n",
    "benign_correct = sum((test_labels_binary == 0) & (test_preds_binary == 0))\n",
    "benign_total = sum(test_labels_binary == 0)\n",
    "malicious_correct = sum((test_labels_binary == 1) & (test_preds_binary == 1))\n",
    "malicious_total = sum(test_labels_binary == 1)\n",
    "\n",
    "# Display results\n",
    "print(f'\\n{\"=\"*70}')\n",
    "print(f'üéØ TEST SET EVALUATION RESULTS')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'Test Size:          {len(test_labels):,} images')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'üìä Overall Metrics:')\n",
    "print(f'  Accuracy:         {accuracy:.4f} ({accuracy*100:.2f}%)')\n",
    "print(f'  Precision:        {precision:.4f}')\n",
    "print(f'  Recall:           {recall:.4f}')\n",
    "print(f'  F1-Score:         {f1:.4f}')\n",
    "print(f'  ROC-AUC:          {roc_auc:.4f}')\n",
    "print(f'  Avg Precision:    {avg_precision:.4f}')\n",
    "print(f'{\"=\"*70}')\n",
    "print(f'üìà Per-Class Performance:')\n",
    "print(f'  Benign:           {benign_correct}/{benign_total} ({benign_correct/benign_total*100:.1f}%)')\n",
    "print(f'  Malicious:        {malicious_correct}/{malicious_total} ({malicious_correct/malicious_total*100:.1f}%)')\n",
    "print(f'{\"=\"*70}\\n')\n",
    "\n",
    "# Classification report\n",
    "print('üìã Detailed Classification Report:')\n",
    "print(classification_report(test_labels_binary, test_preds_binary, \n",
    "                          target_names=['Benign', 'Malicious'], digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e60f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: Confusion Matrix Visualization\n",
    "# ============================================================================\n",
    "\n",
    "cm = confusion_matrix(test_labels_binary, test_preds_binary)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Benign', 'Malicious'],\n",
    "            yticklabels=['Benign', 'Malicious'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "axes[0].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Normalized\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Benign', 'Malicious'],\n",
    "            yticklabels=['Benign', 'Malicious'],\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "axes[1].set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Saved: confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: ROC & Precision-Recall Curves\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(test_labels_binary, test_probs)\n",
    "axes[0].plot(fpr, tpr, color='darkorange', lw=2.5, \n",
    "            label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', \n",
    "            label='Random classifier (AUC = 0.5000)')\n",
    "axes[0].fill_between(fpr, tpr, alpha=0.2, color='darkorange')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc=\"lower right\", fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(test_labels_binary, test_probs)\n",
    "axes[1].plot(recall_curve, precision_curve, color='green', lw=2.5, \n",
    "            label=f'PR curve (AP = {avg_precision:.4f})')\n",
    "axes[1].fill_between(recall_curve, precision_curve, alpha=0.2, color='green')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc=\"lower left\", fontsize=10)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'roc_pr_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Saved: roc_pr_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c30936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: Prediction Distribution Analysis\n",
    "# ============================================================================\n",
    "\n",
    "benign_probs = test_probs[test_labels_binary == 0]\n",
    "malicious_probs = test_probs[test_labels_binary == 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(benign_probs, bins=50, alpha=0.7, label='Benign (True)', \n",
    "            color='blue', edgecolor='black')\n",
    "axes[0].hist(malicious_probs, bins=50, alpha=0.7, label='Malicious (True)', \n",
    "            color='red', edgecolor='black')\n",
    "axes[0].axvline(x=0.5, color='green', linestyle='--', linewidth=2.5, \n",
    "               label='Decision Threshold (0.5)')\n",
    "axes[0].set_xlabel('Predicted Probability (Malicious)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Distribution of Predicted Probabilities', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "data_to_plot = [benign_probs, malicious_probs]\n",
    "bp = axes[1].boxplot(data_to_plot, labels=['Benign\\n(True)', 'Malicious\\n(True)'], \n",
    "                     patch_artist=True,\n",
    "                     boxprops=dict(facecolor='lightblue', edgecolor='black', linewidth=1.5),\n",
    "                     medianprops=dict(color='red', linewidth=2.5),\n",
    "                     whiskerprops=dict(linewidth=1.5),\n",
    "                     capprops=dict(linewidth=1.5))\n",
    "axes[1].axhline(y=0.5, color='green', linestyle='--', linewidth=2, \n",
    "               label='Threshold (0.5)')\n",
    "axes[1].set_ylabel('Predicted Probability (Malicious)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Probability Distribution by True Class', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'prediction_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('‚úÖ Saved: prediction_distribution.png')\n",
    "\n",
    "# Statistics\n",
    "print(f'\\nüìä Probability Statistics:')\n",
    "print(f'Benign (True):')\n",
    "print(f'  Mean: {benign_probs.mean():.4f}, Median: {np.median(benign_probs):.4f}, Std: {benign_probs.std():.4f}')\n",
    "print(f'Malicious (True):')\n",
    "print(f'  Mean: {malicious_probs.mean():.4f}, Median: {np.median(malicious_probs):.4f}, Std: {malicious_probs.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: Save Predictions to CSV\n",
    "# ============================================================================\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'true_label': test_labels_binary,\n",
    "    'true_label_name': ['Benign' if l == 0 else 'Malicious' for l in test_labels_binary],\n",
    "    'predicted_label': test_preds_binary,\n",
    "    'predicted_label_name': ['Benign' if p == 0 else 'Malicious' for p in test_preds_binary],\n",
    "    'probability_malicious': test_probs,\n",
    "    'probability_benign': 1 - test_probs,\n",
    "    'correct': test_labels_binary == test_preds_binary,\n",
    "    'confidence': np.maximum(test_probs, 1 - test_probs)\n",
    "})\n",
    "\n",
    "# Add error analysis\n",
    "predictions_df['prediction_type'] = 'Correct'\n",
    "predictions_df.loc[(predictions_df['true_label'] == 0) & (predictions_df['predicted_label'] == 1), 'prediction_type'] = 'False Positive'\n",
    "predictions_df.loc[(predictions_df['true_label'] == 1) & (predictions_df['predicted_label'] == 0), 'prediction_type'] = 'False Negative'\n",
    "\n",
    "# Save\n",
    "csv_path = os.path.join(OUTPUT_DIR, 'test_predictions.csv')\n",
    "predictions_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f'‚úÖ Saved: test_predictions.csv')\n",
    "print(f'\\nüìä Prediction Summary:')\n",
    "print(predictions_df['prediction_type'].value_counts())\n",
    "print(f'\\nüìÑ First 10 rows:')\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 12: Sample Predictions (Visual Inspection)\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nüì∑ Sample Predictions (Random 15):')\n",
    "print('='*80)\n",
    "\n",
    "sample_indices = random.sample(range(len(test_pairs)), min(15, len(test_pairs)))\n",
    "\n",
    "for idx in sample_indices:\n",
    "    img_path, true_label = test_pairs[idx]\n",
    "    true_label_str = \"Malicious\" if true_label == 1 else \"Benign\"\n",
    "    pred_label = \"Malicious\" if test_preds_binary[idx] == 1 else \"Benign\"\n",
    "    prob = test_probs[idx]\n",
    "    confidence = max(prob, 1 - prob)\n",
    "    correct = \"‚úÖ\" if pred_label == true_label_str else \"‚ùå\"\n",
    "    \n",
    "    print(f'{os.path.basename(img_path)[:55]:55s}')\n",
    "    print(f'  True: {true_label_str:10s} | Pred: {pred_label:10s} | Prob: {prob:.3f} | Conf: {confidence:.1%} {correct}')\n",
    "    print()\n",
    "\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 13: Error Analysis - Show Worst Predictions\n",
    "# ============================================================================\n",
    "\n",
    "print('\\nüîç Error Analysis: Most Confident Mistakes\\n')\n",
    "\n",
    "# Get errors\n",
    "errors_df = predictions_df[predictions_df['correct'] == False].copy()\n",
    "errors_df = errors_df.sort_values('confidence', ascending=False)\n",
    "\n",
    "print(f'Total errors: {len(errors_df)} ({len(errors_df)/len(predictions_df)*100:.2f}%)')\n",
    "print(f'False Positives: {sum(predictions_df[\"prediction_type\"] == \"False Positive\")}')\n",
    "print(f'False Negatives: {sum(predictions_df[\"prediction_type\"] == \"False Negative\")}')\n",
    "\n",
    "if len(errors_df) > 0:\n",
    "    print(f'\\n‚ùå Top 10 Most Confident Errors:')\n",
    "    print('='*80)\n",
    "    for idx, row in errors_df.head(10).iterrows():\n",
    "        print(f'True: {row[\"true_label_name\"]:10s} | '\n",
    "              f'Pred: {row[\"predicted_label_name\"]:10s} | '\n",
    "              f'Confidence: {row[\"confidence\"]:.1%} | '\n",
    "              f'Type: {row[\"prediction_type\"]}')\n",
    "else:\n",
    "    print('\\nüéâ Perfect predictions! No errors found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f'\\n{\"=\"*80}')\n",
    "print(f'‚úÖ EVALUATION COMPLETE!')\n",
    "print(f'{\"=\"*80}')\n",
    "print(f'\\nüìÅ Artifacts saved to: {OUTPUT_DIR}')\n",
    "print(f'   ‚úÖ confusion_matrix.png')\n",
    "print(f'   ‚úÖ roc_pr_curves.png')\n",
    "print(f'   ‚úÖ prediction_distribution.png')\n",
    "print(f'   ‚úÖ test_predictions.csv')\n",
    "print(f'\\nüìä Key Results:')\n",
    "print(f'   Accuracy:  {accuracy*100:.2f}%')\n",
    "print(f'   Precision: {precision:.4f}')\n",
    "print(f'   Recall:    {recall:.4f}')\n",
    "print(f'   F1-Score:  {f1:.4f}')\n",
    "print(f'   ROC-AUC:   {roc_auc:.4f}')\n",
    "print(f'\\nüí° Next Steps:')\n",
    "print(f'   1. Download the visualizations from the Output section')\n",
    "print(f'   2. Review test_predictions.csv for detailed analysis')\n",
    "print(f'   3. Check the error analysis above for model weaknesses')\n",
    "print(f'{\"=\"*80}')\n",
    "print(f'\\nüéâ Well done! Your model evaluation is complete.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
